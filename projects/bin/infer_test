#!/usr/bin/env python
import sys
import os
import yaml
import argparse
from semantic_selector import ml_model
from semantic_selector import chainer_model
from semantic_selector import datasource



def main():
    '''
        ./bin/infer_test
    '''

    parser = argparse.ArgumentParser(description='')
    parser.add_argument('--threashold', type=int, nargs='?', help='a threashold of the number of labels', default=10)
    parser.add_argument('--ratio_test', type=float, nargs='?', help='a ratio of test sets', default=0.05)
    args = parser.parse_args()

    (training, tests) = datasource.InputTags(args.threashold).fetch_data(args.ratio_test)
    model = chainer_model.ChainerModel(training)

    print("failing inferences\n")
    print("estimated, correct")
    positive_hit = 0
    unknown_hit = 0
    unknown_cnt = 0
    for t in tests:
        target_tag = t.html
        estimated_label = model.inference_html(target_tag)
        correct_label = t.label
        if estimated_label != correct_label:
            print(estimated_label + "," + correct_label)
        if correct_label == 'unknown':
            unknown_cnt += 1
        if estimated_label == correct_label:
            if correct_label == 'unknown':
                unknown_hit += 1
            else:
                positive_hit += 1

    print()
    print("# of test data: " + str(len(tests)))
    print("# of training_data: " + str(model.n))
    print("model description: " + model.describe())
    print("Accuracy,", ((positive_hit + unknown_hit) / len(tests)))
    print("Recall,", (positive_hit / (len(tests) - unknown_cnt)))
    print("unkown ratio in test data,", (unknown_cnt / len(tests)))


if __name__ == '__main__':
    main()
