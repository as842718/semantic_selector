#!/usr/bin/env python
import sys
import os
import yaml
import argparse
from semantic_selector import nn_fc_model
from semantic_selector import datasource


def main():
    '''
        ./bin/infer_test
    '''

    parser = argparse.ArgumentParser(description='')
    parser.add_argument('--threashold', type=int, nargs='?', help='a threashold of the number of labels', default=10)
    parser.add_argument('--ratio_test', type=float, nargs='?', help='a ratio of test sets', default=0.05)
    args = parser.parse_args()

    (training, tests) = datasource.InputTags(args.threashold).fetch_data(args.ratio_test)
    model = nn_fc_model.NNFullyConnectedModel(training, tests)
    print("failing inferences\n")
    print("estimated, correct")
    positive_hit = 0
    unknown_hit = 0
    unknown_cnt = 0
    for t in tests:
        target_tag = t.html
        estimated_label = model.inference_html(t)
        correct_label = t.label
        if estimated_label != correct_label:
            print(estimated_label + "," + correct_label)
        if correct_label == 'unknown':
            unknown_cnt += 1
        if estimated_label == correct_label:
            if correct_label == 'unknown':
                unknown_hit += 1
            else:
                positive_hit += 1

    print()
    print("# of test data: " + str(len(tests)))
    #print("# of training_data: " + str(len(model.word_vecs)))
    #print("# of vector elements: " + str(model.num_topics))
    #print("Model Fitting Score,", model.fitting_score)
    print("Accuracy,", ((positive_hit + unknown_hit) / len(tests)))
    print("Recall,", (positive_hit / (len(tests) - unknown_cnt)))
    print("unkown ratio in test data,", (unknown_cnt / len(tests)))


if __name__ == '__main__':
    main()
